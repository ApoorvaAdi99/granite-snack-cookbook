{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) with LlamaIndex\n",
    "*Using IBM Granite Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recipe Overview\n",
    "\n",
    "Welcome to this Granite Recipe!\n",
    "\n",
    "In this notebook you will learn to implement Retrieval Augumented Generation (RAG) using LlamaIndex orchestration framework. \n",
    "\n",
    "RAG is an architecture that optimizes the performance of language models by connecting it to knowledge bases. By doing so, the language models are capable of recalling factual information from the knowledge base and customizing this information to respond to the user query. \n",
    "\n",
    "The major components of RAG architecture are:\n",
    "1. Knowledge Base - Data repository for the system\n",
    "2. Retriever - A language model that gathers context from the knowledge base that is relevant to the user query\n",
    "3. Generator - A language model that generates response to the augmented query that contains the user query and the context identified by the retriever\n",
    "4. Integration Layer - A layer that co-ordinates and brings together the functionality of all the components\n",
    "\n",
    "Advantages of RAG architecture include access to domain-specific information, cost efficient AI implementation/scaling, reduced risk of hallucinations, greater data security etc. Some use cases of RAG are:\n",
    "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
    "- Specialized chatbot: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
    "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
    "\n",
    "[![Open YouTube video](https://img.youtube.com/vi/T-D1OfcDW1M/0.jpg)](https://www.youtube.com/watch?v=T-D1OfcDW1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    transformers \\\n",
    "    llama-index \\\n",
    "    llama-index-embeddings-huggingface \\\n",
    "    llama-index-vector-stores-chroma \\\n",
    "    wget \\\n",
    "    chromadb \\\n",
    "    llama-index-llms-replicate \\\n",
    "    replicate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Components Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Selection (Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the embedding model and the tokenizer for the architecture. The embedding model generates vector representations of the user query and knowledge base, enabling retrieval of semantically relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbedding(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Selection (Generator)\n",
    "\n",
    "The LLM will be the generator component that answers the user query, given the retrieved context. For this recipe, we connect to Granite 3.3 8B model using LlamaIndex-Replicate client.\n",
    "\n",
    "You can select other Granite models from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.replicate import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the global parameters, we ensure consistency across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = model\n",
    "Settings.embedding = embeddings_model\n",
    "Settings.chunk_size = embeddings_tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Selection (Knowledge Base)\n",
    "\n",
    "Identify the database to store and retrieve embedding vectors.\n",
    "In this recipe, we select ChromaDB to store our Knowledge Base. The storage of knowledge base in the form of vectors help in efficient similarity computation and relevant context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"granite_rag_collection\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vector Database\n",
    "\n",
    "In this recipe, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the document\n",
    "\n",
    "Here we use President Biden's State of the Union address from March 1, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "  wget.download(url, out=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "Split the document into text segments that can fit into the model's context window.\n",
    "\n",
    "Please note that the chunk size is set to model's context window under the Global Settings section and is implicitly passed to SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[filename]).load_data()\n",
    "sentence_splitter = SentenceSplitter(chunk_overlap=0)\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.metadata[\"doc_id\"] = idx\n",
    "\n",
    "print(f\"{len(nodes)} text document chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Populate the vector database\n",
    "\n",
    "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=embeddings_model,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval from Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a similarity search\n",
    "\n",
    "Search the knowledge base for similar documents by calculating the proximity of embedded vectors of the query and the documents in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Fortune 500 Corporations?\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "retrieval_results = retriever.retrieve(query)\n",
    "print(f\"{len(retrieval_results)} documents returned\")\n",
    "for i, node in enumerate(retrieval_results):\n",
    "    print(f\"\\nDocument {i+1} :\")\n",
    "    print(f\"\\nDocument ID : {node.metadata['doc_id']}\")\n",
    "    print(f\"\\nScore {i+1} : {node.score:.2f}\")\n",
    "    print(f\"\\nText:\\n\")\n",
    "    print(node.text)\n",
    "    print(\"=\" * 80)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "\n",
    "This section of the recipe builds a pipeline to generate response for a RAG query. Below are the steps implemented under the pipeline:\n",
    "1. **Prompt Template**- Create the question-answering system prompt for Granite models that includes the user query and the retreived context.\n",
    "\n",
    "2. **Response Synthesizer** - Use a response synthesizer with system prompt as input and generate the response using the language model. The response synthesizer can have different modes. To state a few,\n",
    "    - Refine : Takes each chunk of the user query sequentially and improves the response\n",
    "    - Accumulate : Accumulates responses from multiple text chunks\n",
    "    - Tree Summarize : Builds a tree index over the set of candidate nodes, with a summary prompt seeded with the query. The tree is built in a bottoms-up fashion, and in the end the root node is returned as the response\n",
    " \n",
    "    In this recipe, we use Compact and Refine - an approach that refines responses over compact chunks of texts.\n",
    "\n",
    "3. **Query Engine** - Query Engine is an interface that allows the user to ask questions. This component combines the synthesizer and the retreiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import RichPromptTemplate\n",
    "from llama_index.core.response_synthesizers import Refine\n",
    "\n",
    "prompt_template = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{{query}}\"\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"\",  \n",
    "        \"text\": \"{{context}}\"\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "template_var_mappings = {\"context_str\": \"context\", \"query_str\": \"query\"}\n",
    "qa_prompt_template = RichPromptTemplate(\n",
    "    prompt_template, template_var_mappings=template_var_mappings\n",
    ")\n",
    "\n",
    "response_synthesizer = Refine(\n",
    "    llm=model,\n",
    "    text_qa_template=qa_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Response Generation using RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The query can now be passed to the Query Engine and the RAG pipeline is executed to generate the response based on the context gathered by the Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "query = \"What was said about Ketanji Brown Jackson's nomination to the Supreme Court?\"\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(\"=== RAG Response ===\")\n",
    "print(wrap_text(answer.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source documents that were identified as relevant context can be observed using the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n=== Source Documents ===\")\n",
    "for i, source_node in enumerate(answer.source_nodes):\n",
    "    doc_id = source_node.metadata.get('doc_id', 'N/A')\n",
    "    print(f\"\\nDocument {i+1} :\")\n",
    "    print(f\"\\nDocument ID : {doc_id}\")\n",
    "    print(f\"\\nScore {i+1} : {source_node.score:.2f}\")\n",
    "    print(f\"\\nText:\\n\")\n",
    "    print(source_node.text)\n",
    "    print(\"=\" * 80)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries outside the scope of Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries beyond the scope of the knowledge base will not be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "query = \"When was the last time Ferrari won the Formula 1 World Championship?\"\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(\"=== RAG Response ===\")\n",
    "print(wrap_text(answer.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this recipe covers the implementation of simple RAG architecture using LlamaIndex orchestration layer and a knowledge base stored in ChromaDB. We use the LlamaIndex-Replicate client for the Granite language model and LlamaIndex-HuggingFace client for the Granite tokenizers. \n",
    "\n",
    "For more recipes on RAG architectures, please refer [here](https://github.com/ibm-granite-community/granite-snack-cookbook/tree/main/recipes/RAG). You can also explore more on Agentic RAG in this [recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_RAG.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "1. “What is retrieval-augmented generation?”. 2023. IBM Research Blog. https://research.ibm.com/blog/retrieval-augmented-generation-RAG.\n",
    "2. \"Basic Chat Template Examples\". 2025. IBM Granite Documentation. https://www.ibm.com/granite/docs/models/granite/#basic-chat-template-example. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
